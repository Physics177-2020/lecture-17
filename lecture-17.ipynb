{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17: Newton's method\n",
    "\n",
    "In the last lecture we saw that, even after implementing a reasonable [line search](https://en.wikipedia.org/wiki/Line_search) method, it can take a long time for gradient descent algorithms to approach the minimum of a function. One example where this can happen is in multidimensional functions where the curvature along different dimensions is significantly unbalanced. \n",
    "\n",
    "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) can help us to get around this specific problem by taking the curvature of the function into account. This method works by expanding the function $f(\\underline{x})$ to second order,\n",
    "\n",
    "$$\n",
    "f(\\underline{x} + \\underline{s}) = f(\\underline{x}) + \\underline{s}^T\\nabla f(\\underline{x}) + \\underline{s}^T \\nabla^2 f(\\underline{x}) \\underline{s}\\,.\n",
    "$$\n",
    "\n",
    "We can then solve for the step direction $\\underline{s}$ that minimizes the function. The resulting step direction is called the **Newtwon direction**,\n",
    "\n",
    "$$\n",
    "\\underline{s}_N = -\\left[\\nabla^2 f(\\underline{x})\\right]^{-1} \\nabla f(\\underline{x})\\,.\n",
    "$$\n",
    "\n",
    "Note that this direction is different from the steepest descent direction! One interesting feature of Newton's method is that, unlike steepest descent, the equation above sets a \"natural\" step length. \n",
    "\n",
    "\n",
    "### Example: Optimizing an unbalanced quadratic function\n",
    "\n",
    "Let's return to the example that gave us trouble last time: a two-dimensional quadratic function, \n",
    "\n",
    "$$\n",
    "f(\\underline{x}) = a_1 x_1^2 + a_2 x_2^2\\,,\n",
    "$$\n",
    "\n",
    "with $a_1 = 1$ and $a_2 = 1000$. This function is clearly imbalanced -- deviations from zero are penalized much more strongly for the second dimension than for the first. As a result, it becomes harder to effectively \"move\" the parameters along the first dimension with gradient descent. Let's begin to explore this by plotting the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the quadratic function\n",
    "\n",
    "a = np.array([1., 1000.])\n",
    "\n",
    "def f(x):\n",
    "    return np.sum(a * (x**2))\n",
    "\n",
    "\n",
    "# Plot the function\n",
    "\n",
    "delta = 0.025\n",
    "x = np.arange(-30.0, 30.0, delta)\n",
    "y = np.arange(-1.0, 1.0, delta)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (a[0]*X**2) + (a[1]*Y**2)\n",
    "\n",
    "plt.contour(X, Y, Z)\n",
    "plt.xticks([-30, -20, -10, 0, 10, 20, 30])\n",
    "plt.yticks([-1, 0, 1])\n",
    "plt.xlabel('dimension 1')\n",
    "plt.ylabel('dimension 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to go a huge distance in the first dimension to change the function by the same amount as we get if we go just a small distance in the second dimension. This means that the derivatives are dominated by the second dimension, since they are perpendicular to the [level sets](https://en.wikipedia.org/wiki/Level_set) of the function in the above plot.\n",
    "\n",
    "### Comparing the step directions\n",
    "\n",
    "Let's examine this more carefully by explicitly computing the steepest descent direction, starting from the point $(0.9, 0.9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the derivative function\n",
    "\n",
    "def df(x):\n",
    "    return 2*a*x\n",
    "\n",
    "\n",
    "# Compute the steepest descent direction\n",
    "\n",
    "x0 = np.array([0.9, 0.9])\n",
    "s  = # FILL THIS IN\n",
    "\n",
    "print('The steepest descent direction is {}'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the steepest descent direction is pointed almost entirely along the second dimension. If we update the parameters in this direction, we will make little progress toward the minimum along the first dimension.\n",
    "\n",
    "Now, let's compute the Newton direction. This is more complicated because now we need to compute the **matrix** of second derivatives of our function (sometimes called the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)), take its inverse, and multiply by the vector of derivatives:\n",
    "\n",
    "$$\n",
    "\\underline{s}_N = -\\left[\\nabla^2 f(\\underline{x})\\right]^{-1} \\nabla f(\\underline{x})\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the matrix of second derivatives\n",
    "\n",
    "def ddf(x):\n",
    "    return np.diag(2*a) # diagonal 2x2 matrix with 2a on the diagonal!\n",
    "\n",
    "\n",
    "# Now compute the Newton direction\n",
    "\n",
    "x0  = np.array([0.9, 0.9])\n",
    "s_N = # FILL THIS IN\n",
    "\n",
    "print('The Newton direction is {}'.format(s_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the function is actually quadratic, Newton's method points **precisely** toward the minimum of the function! More generally, when it can be calculated, the inclusion of curvature in Newton's method can greatly speed convergence. This is especially true in the neighborhood of the minimum (when we are close enough to the minimum, all functions can be well-approximated as quadratic).\n",
    "\n",
    "### Example: The Rosenbrock function\n",
    "\n",
    "The [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) is a **non-convex** function that was originally designed for testing optimization algorithms. It is challenging to optimize for the same reasons as the quadratic function above -- the minimum lies in a long, shallow \"valley\" which is difficult to traverse by following the gradient. It is defined as \n",
    "\n",
    "$$\n",
    "f(\\underline{x}) = 100(x_2 − x_1^2)^2 + (1 − x_1)^2\\,.\n",
    "$$\n",
    "\n",
    "First, let's visualize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function\n",
    "\n",
    "def f(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "\n",
    "# Plot the function\n",
    "\n",
    "delta = 0.005\n",
    "x = np.arange(0, 2.0, delta)\n",
    "y = np.arange(0, 2.0, delta)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.log(100 * (Y - X**2)**2 + (1 - X)**2 + 0.1)\n",
    "\n",
    "plt.contour(X, Y, Z)\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.xlabel('dimension 1')\n",
    "plt.ylabel('dimension 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we started at the point $(1.5, 1.5)$ and tried to use steepest descent? Let's compute the steepest descent direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the derivative\n",
    "\n",
    "def df(x):\n",
    "    return np.array([-400 * (x[1] - x[0]**2) * x[0] - 2 * (1 - x[0]), 200 * (x[1] - x[0]**2)])\n",
    "\n",
    "\n",
    "# And compute the step direction\n",
    "\n",
    "x0 = np.array([1.5, 1.5])\n",
    "s  = df(x0)\n",
    "\n",
    "print('The steepest descent direction is {}'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running gradient descent on the Rosenbrock function\n",
    "\n",
    "Let's import our gradient descent algorithm -- including the line search -- from last lecture and attempt to minimize the Rosenbrock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set line search parameters\n",
    "\n",
    "beta1 = 0.4    # Step size multiplier if sufficient decrease fails\n",
    "beta2 = 1.2    # Step size multiplier if curvature condition fails\n",
    "alpha = 0.001  # Sufficient decrease parameter\n",
    "gamma = 0.5    # Curvature condition parameter\n",
    "\n",
    "\n",
    "# Set steepest descent parameters\n",
    "\n",
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "\n",
    "# Initialize and iteratre\n",
    "\n",
    "x0   = np.array([1.5, 1.5]) # Starting value of parameter\n",
    "x    = x0                   # Current value of the parameter\n",
    "dfdx = df(x0)               # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx1\\tx2\\tf(x)\\tdf/dx1\\tdf/dx2')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.sum(np.fabs(dfdx))>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' % (it, x[0], x[1], f(x), dfdx[0], dfdx[1]))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = -df(x)\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t = 1 \n",
    "    both_passed = False\n",
    "    \n",
    "    while not both_passed:\n",
    "\n",
    "        # Check for sufficient decrease fail\n",
    "\n",
    "        if f(x + (t*s)) > f(x) + (alpha * t * np.dot(df(x), s)):\n",
    "            t = beta1 * t\n",
    "\n",
    "        # If passed, check for curvature condition fail\n",
    "\n",
    "        elif np.dot(df(x + (t*s)), s) < gamma * np.dot(df(x), s):\n",
    "            t = beta2 * t\n",
    "\n",
    "        # If both passed, exit the loop\n",
    "\n",
    "        else:\n",
    "            both_passed = True\n",
    "    \n",
    "    # Update the parameters\n",
    "    x = x + t*s\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = df(x)\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "print('\\nFound the minimum near x* = (%lf, %lf), true minimum is (1, 1)' % (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, we get stuck in the long, shallow basin and we stop making progress.\n",
    "\n",
    "### Newton's method for the Rosenbrock function\n",
    "\n",
    "Let's compare this result with the answer from Newton's method. To do this we need to compute the matrix of second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the matrix of second derivatives\n",
    "\n",
    "def ddf(x):\n",
    "    return np.array([[2 + 800*x[0]**2 - 400*(x[1] - x[0]**2), -400*x[0]], [-400*x[0], 200]])\n",
    "\n",
    "\n",
    "# Now compute the Newton direction\n",
    "\n",
    "x0  = np.array([0.7, 1.3])\n",
    "s_N = - np.matmul( np.linalg.inv(ddf(x0)), df(x0) ) # matrix multiplication\n",
    "\n",
    "print('The Newton direction is {}'.format(s_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the step size differs dramatically. Though it is unbalanced, we can observe convergence toward the optimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "x0   = np.array([1.5, 1.5])  # Starting value of parameter\n",
    "x    = x0                    # Current value of the parameter\n",
    "dfdx = df(x0)                # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx1\\tx2\\tf(x)\\tdf/dx1\\tdf/dx2')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.sum(np.fabs(dfdx))>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f' % (it, x[0], x[1], f(x), dfdx[0], dfdx[1]))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = -np.matmul( np.linalg.inv(ddf(x)), df(x) )\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t  = 1\n",
    "    \n",
    "    # Update the parameters\n",
    "    x += t * s\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = df(x)\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "print('\\nFound the minimum near x* = (%lf, %lf), true minimum is (1, 1)' % (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
